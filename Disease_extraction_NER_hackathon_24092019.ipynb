{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Importing-Libraries\" data-toc-modified-id=\"Importing-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Importing Libraries</a></span></li><li><span><a href=\"#Reading-Data\" data-toc-modified-id=\"Reading-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reading Data</a></span></li><li><span><a href=\"#Creating-Word-&amp;-Tag-dictionary\" data-toc-modified-id=\"Creating-Word-&amp;-Tag-dictionary-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Creating Word &amp; Tag dictionary</a></span></li><li><span><a href=\"#Getting-Train-&amp;-Test-Sentences\" data-toc-modified-id=\"Getting-Train-&amp;-Test-Sentences-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Getting Train &amp; Test Sentences</a></span></li><li><span><a href=\"#Feature-Extraction-for-DL-Model\" data-toc-modified-id=\"Feature-Extraction-for-DL-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature Extraction for DL Model</a></span></li><li><span><a href=\"#Building-Bidirectional-LSTM-Model\" data-toc-modified-id=\"Building-Bidirectional-LSTM-Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Building Bidirectional LSTM Model</a></span></li><li><span><a href=\"#Prediction-on-Test-Set\" data-toc-modified-id=\"Prediction-on-Test-Set-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Prediction on Test Set</a></span></li><li><span><a href=\"#Prepare-Submission-Data\" data-toc-modified-id=\"Prepare-Submission-Data-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Prepare Submission Data</a></span></li><li><span><a href=\"#Writing-the-Submission-File\" data-toc-modified-id=\"Writing-the-Submission-File-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Writing the Submission File</a></span></li><li><span><a href=\"#LeaderBoard-Score\" data-toc-modified-id=\"LeaderBoard-Score-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>LeaderBoard Score</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Innoplexus Online Hiring Hackathon \n",
    "\n",
    "# Saving lives with AI : Disease Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import unicodedata\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "# Defining Constants\n",
    "\n",
    "# Maximum length of text sentences\n",
    "MAXLEN = 180\n",
    "# Number of LSTM units\n",
    "LSTM_N = 150\n",
    "# batch size\n",
    "BS=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagged_sentences(data): \n",
    "    '''\n",
    "    Objective: To get list of sentences along with labelled tags.\n",
    "    Returns a list of lists of (word,tag) tuples. \n",
    "    Each inner list contains a words of a sentence along with tags. \n",
    "    '''\n",
    "    \n",
    "    agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(), s[\"tag\"].values.tolist())] \n",
    "    grouped = data.groupby(\"Sent_ID\").apply(agg_func) \n",
    "    sentences = [s for s in grouped] \n",
    "    return sentences \n",
    "\n",
    "def get_test_sentences(data): \n",
    "    '''\n",
    "    Objective: To get list of sentences.\n",
    "    Returns a list of lists of words. \n",
    "    Each inner list contains a words of a sentence.\n",
    "    '''\n",
    "    \n",
    "    agg_func = lambda s: [w for w in s[\"Word\"].values.tolist()] \n",
    "    grouped = data.groupby(\"Sent_ID\").apply(agg_func) \n",
    "    sentences = [s for s in grouped] \n",
    "    return sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Sent_ID</th>\n",
       "      <th>Word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Obesity</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Low-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Middle-Income</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Countries</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Burden</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Drivers</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Doc_ID  Sent_ID           Word tag\n",
       "0   1       1        1        Obesity   O\n",
       "1   2       1        1             in   O\n",
       "2   3       1        1           Low-   O\n",
       "3   4       1        1            and   O\n",
       "4   5       1        1  Middle-Income   O\n",
       "5   6       1        1      Countries   O\n",
       "6   7       1        1              :   O\n",
       "7   8       1        1         Burden   O\n",
       "8   9       1        1              ,   O\n",
       "9  10       1        1        Drivers   O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the training set\n",
    "data = pd.read_csv(\"train.csv\", encoding=\"latin1\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Sent_ID</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4543834</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>CCCVA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4543835</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4543836</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>MANOVA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4543837</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4543838</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4543839</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4543840</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>hen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4543841</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4543842</td>\n",
       "      <td>30001</td>\n",
       "      <td>191284</td>\n",
       "      <td>Comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4543843</td>\n",
       "      <td>30001</td>\n",
       "      <td>191284</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  Doc_ID  Sent_ID      Word\n",
       "0  4543834   30001   191283     CCCVA\n",
       "1  4543835   30001   191283         ,\n",
       "2  4543836   30001   191283    MANOVA\n",
       "3  4543837   30001   191283         ,\n",
       "4  4543838   30001   191283        my\n",
       "5  4543839   30001   191283     black\n",
       "6  4543840   30001   191283       hen\n",
       "7  4543841   30001   191283         .\n",
       "8  4543842   30001   191284  Comments\n",
       "9  4543843   30001   191284        on"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the test set\n",
    "test_data = pd.read_csv(\"test.csv\", encoding=\"latin1\")\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Word & Tag dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of uniques docs, sentences and words in Training set:\n",
      " id         4543833\n",
      "Doc_ID       30000\n",
      "Sent_ID     191282\n",
      "Word        184505\n",
      "tag              3\n",
      "dtype: int64\n",
      "\n",
      "Number of uniques docs, sentences and words in Test set:\n",
      " id         2994463\n",
      "Doc_ID       20000\n",
      "Sent_ID     125840\n",
      "Word        139891\n",
      "dtype: int64\n",
      "\n",
      "Length of vocabulary =  257203\n",
      "\n",
      "number of tags =  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of uniques docs, sentences and words in Training set:\\n\",data.nunique())\n",
    "print(\"\\nNumber of uniques docs, sentences and words in Test set:\\n\",test_data.nunique())\n",
    "\n",
    "# Creating a vocabulary\n",
    "words = list(set(data[\"Word\"].append(test_data[\"Word\"]).values))\n",
    "words.append(\"ENDPAD\")\n",
    "\n",
    "# Converting greek characters to ASCII characters eg. 'naïve café' to 'naive cafe' \n",
    "words = [unicodedata.normalize('NFKD', str(w)).encode('ascii','ignore') for w in words]\n",
    "n_words = len(words)\n",
    "print(\"\\nLength of vocabulary = \",n_words)\n",
    "\n",
    "tags = list(set(data[\"tag\"].values))\n",
    "n_tags = len(tags)\n",
    "print(\"\\nnumber of tags = \",n_tags)\n",
    "\n",
    "# Creating words to indices dictionary.\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "# Creating tags to indices dictionary.\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Train & Test Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 sentences in a word list format:\n",
      " [[('Obesity', 'O'), ('in', 'O'), ('Low-', 'O'), ('and', 'O'), ('Middle-Income', 'O'), ('Countries', 'O'), (':', 'O'), ('Burden', 'O'), (',', 'O'), ('Drivers', 'O'), (',', 'O'), ('and', 'O'), ('Emerging', 'O'), ('Challenges', 'O'), ('.', 'O')], [('We', 'O'), ('have', 'O'), ('reviewed', 'O'), ('the', 'O'), ('distinctive', 'O'), ('features', 'O'), ('of', 'O'), ('excess', 'O'), ('weight', 'O'), (',', 'O'), ('its', 'O'), ('causes', 'O'), (',', 'O'), ('and', 'O'), ('related', 'O'), ('prevention', 'O'), ('and', 'O'), ('management', 'O'), ('efforts', 'O'), (',', 'O'), ('as', 'O'), ('well', 'O'), ('as', 'O'), ('data', 'O'), ('gaps', 'O'), ('and', 'O'), ('recommendations', 'O'), ('for', 'O'), ('future', 'O'), ('research', 'O'), ('in', 'O'), ('low-', 'O'), ('and', 'O'), ('middle-income', 'O'), ('countries', 'O'), ('(', 'O'), ('LMICs', 'O'), (')', 'O'), ('.', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "# Getting training setences in a list\n",
    "sentences = get_tagged_sentences(data) \n",
    "print(\"First 2 sentences in a word list format:\\n\",sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 sentences in a word list format:\n",
      " [['CCCVA', ',', 'MANOVA', ',', 'my', 'black', 'hen', '.'], ['Comments', 'on', 'repeated', 'measures', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Getting test sentences in a list\n",
    "test_sentences = get_test_sentences(test_data) \n",
    "print(\"First 2 sentences in a word list format:\\n\",test_sentences[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting words to indices for test sentences (Features)\n",
    "# Converting greek characters to ASCII characters in train set eg. 'naïve café' to 'naive cafe' \n",
    "X = [[word2idx[unicodedata.normalize('NFKD', str(w[0])).\n",
    "               encode('ascii','ignore')] for w in s] for s in sentences]\n",
    "\n",
    "# Converting words to indices for test sentences (Features)\n",
    "# Converting greek characters to ASCII characters in test-set eg. 'naïve café' to 'naive cafe' \n",
    "X_test = [[word2idx[unicodedata.normalize('NFKD', str(w)).\n",
    "                    encode('ascii','ignore')] for w in s] for s in test_sentences]\n",
    "\n",
    "'''\n",
    "Padding train and test sentences to 180 words.\n",
    "Sentences of length greater than 180 words are truncated.\n",
    "Sentences of length less than 180 words are padded with a high value.\n",
    "''' \n",
    "X = pad_sequences(maxlen=MAXLEN, sequences=X, padding=\"post\", value=n_words - 1)\n",
    "X_test = pad_sequences(maxlen=MAXLEN, sequences=X_test, padding=\"post\", value=n_words - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting tags to indices for test sentences (labels)\n",
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "\n",
    "# Padding tag labels to 180 words.\n",
    "y = pad_sequences(maxlen=MAXLEN, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "\n",
    "# Making labels in one hot encoded form for DL model \n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Bidirectional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181717 samples, validate on 9565 samples\n",
      "Epoch 1/2\n",
      "181717/181717 [==============================] - 1040s 6ms/step - loss: 0.0074 - acc: 0.9981 - val_loss: 0.0040 - val_acc: 0.9988\n",
      "Epoch 2/2\n",
      " 97152/181717 [===============>..............] - ETA: 8:07 - loss: 0.0029 - acc: 0.9990"
     ]
    }
   ],
   "source": [
    "# 180 dimensional word indices as input\n",
    "input = Input(shape=(MAXLEN,))\n",
    "\n",
    "# Embedding layer of same length output (180 dim embedding will be generated)\n",
    "model = Embedding(input_dim=n_words, output_dim=MAXLEN, input_length=MAXLEN)(input)\n",
    "\n",
    "# Adding dropout layer\n",
    "model = Dropout(0.2)(model)\n",
    "\n",
    "# Bidirectional LSTM to learn from both forward as well as backward context\n",
    "model = Bidirectional(LSTM(units=LSTM_N, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "\n",
    "# Adding a TimeDistributedDense, to applying a Dense layer on each 180 timesteps\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
    "\n",
    "model = Model(input, out)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X, np.array(y), batch_size=BS, epochs=2, validation_split=0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on trained model\n",
    "pred = model.predict(X_test)\n",
    "print(\"Predicted Probabilities on Test Set:\\n\",pred.shape)\n",
    "# Taking tag class with maximum probability\n",
    "pred_index = np.argmax(pred, axis=-1)\n",
    "print(\"Predicted tag indices: \\n\",pred_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten both the features and predicted tags for submission\n",
    "ids,tagids = X_test.flatten().tolist(), pred_index.flatten().tolist()\n",
    "\n",
    "# converting each word indices back to words\n",
    "words_test = [words[ind].decode('utf-8') for ind in ids]\n",
    "# converting each predicted tag indices back to tags\n",
    "tags_test = [tags[ind] for ind in tagids]\n",
    "print(\"Length of words in Padded test set:\",len(words_test))\n",
    "print(\"Length of tags in Padded test set:\",len(tags_test))\n",
    "print(\"\\nCheck few of words and predicted tags:\\n\",words_test[:10],tags_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Submission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The task here is to convert padded fixed 180 dimensional predicted tags \n",
    "to variable length test set sentences.\n",
    "1. If the sentences have word length shorter than 180, \n",
    "   then predcited tags are skipped.\n",
    "2. If the sentences have word length longer than 180, \n",
    "   then all extra words are tagged with \"O\" tag class.\n",
    "'''\n",
    "\n",
    "i=0\n",
    "j=1\n",
    "predicted_tags = []\n",
    "counts = test_data.groupby('Sent_ID')['id'].count().tolist()\n",
    "\n",
    "for index,count in enumerate(counts):\n",
    "    if count <= MAXLEN:\n",
    "        predicted_tags.append(tags_test[i:i+count])\n",
    "    else:\n",
    "        predicted_tags.append(tags_test[i:i+MAXLEN])\n",
    "        out = ['O']*(count-MAXLEN)\n",
    "        predicted_tags.append(out)\n",
    "       \n",
    "    i=j*MAXLEN\n",
    "    j=j+1\n",
    "    \n",
    "predictions_final = [item for sublist in predicted_tags for item in sublist]    \n",
    "print(\"\\nLength of test set words and predicted tags should match.\")            \n",
    "print(\"Length of predicted tags:\",len(predictions_final))\n",
    "print(\"Length of words in test set:\",test_data['Word'].size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample_submission.csv\", encoding=\"latin1\")\n",
    "# Creating a dataframe in the submission format\n",
    "df_results = pd.DataFrame({'id':df['id'],'Sent_ID':df['Sent_ID'],'tag':predictions_final})\n",
    "# writing submission csv file\n",
    "df_results.to_csv('submission_final.csv',sep=\",\", index=None)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeaderBoard Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relaxed/Partial F1 score on private leaderboard was 77.8%\n",
    "# Partial F1 score: F1 score with considering partial disease name detection\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='/home/abhijeet/Pictures/F1_score.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
